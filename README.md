# Kaggle: LMSYS-Chatbot Arena Human Preference Predictions

**竞赛时长**: 2024年5月 - 2024年8月  
**角色**: 团队主力成员  
**链接**: [比赛介绍](https://www.kaggle.com/competitions/lmsys-chatbot-arena) | [排行榜](https://www.kaggle.com/competitions/lmsys-chatbot-arena/leaderboard)  

---

> 在本次竞赛中，我们团队表现出色，最终取得了 **0.98739** 的分数，荣获银牌（前 **2%**）。

## 概述

### 研究背景

本次竞赛的挑战是预测用户在两个人工智能对话系统（大型语言模型，LLMs）之间的对决中更偏爱哪个回答。参赛者将获得一组来自 Chatbot Arena 的对话数据，这些对话由不同的 LLM 生成。目标是开发一个优秀的机器学习模型，以改进聊天机器人与人类互动的方式，使其更符合人类的偏好。

## 数据集描述

比赛数据集由 ChatBot Arena 的用户交互组成。在每次用户交互中，评判员向两个不同的大型语言模型提供一个或多个提示，然后指出哪个模型给出了更令人满意的回应。比赛的目标是预测评判员的偏好，并确定给定的提示/回应对被选为获胜者的可能性。

[ChatBot Arena 网址](https://arena.lmsys.org/)

![ChatBot Arena 网站截图](images/image.png)

**注意**：这是一个代码竞赛。当您的提交被评分时，这个示例测试数据将被完整的测试集替换。训练数据中有 55K 行，测试集中预计有大约 25,000 行。

### 文件说明

- **train.csv**:
  - `id` - 行的唯一标识符。
  - `model_[a/b]` - model_[a/b] 的身份。包含在 train.csv 中但不在 test.csv 中。
  - `prompt` - 作为输入给出的提示（给两个模型）。
  - `response_[a/b]` - model_[a/b] 对给定提示的回应。
  - `winner_model_[a/b/tie]` - 标记评判选择的二进制列。真实目标列。

- **test.csv**:
  - `id`
  - `prompt`
  - `response_[a/b]`

- **sample_submission.csv**:
  - 一个正确格式的提交文件。
  - `id`
  - `winner_model_[a/b/tie]` - 这是从测试集预测的内容。

**注意**：此竞赛的数据集包含可能被认为是亵渎、粗俗或冒犯的文本。

## 任务描述

参赛者需要基于 `train.csv` 数据集进行模型训练，并对未公开的 `test.csv` 数据集进行预测。预测目标包含三个类别：

- `winner_model_a`: 模型 A 的回答更符合人类偏好
- `winner_model_b`: 模型 B 的回答更符合人类偏好
- `winner_tie`: 两个模型的回答同等优秀或同等不佳

预测结果应为每个类别的概率，且三个概率之和必须等于 1。

## 评估指标

本竞赛采用 **对数损失（Log Loss）** 作为评估指标。它适用于需要输出概率的多类别分类任务，衡量预测概率分布与真实标签分布之间的差异。

### Log Loss 计算公式

对于单个样本：

$$\text{LogLoss} = -\sum(y_i \cdot \log(p_i))$$

其中：

- $$y_i$$ 是真实标签的 one-hot 编码
- $$p_i$$ 是预测的概率

### Log Loss 特点

- **惩罚性强**：错误预测会受到严厉惩罚，尤其是高置信度的错误预测。
  
  数学描述：

  - 对于正确预测（y = 1 时 p → 1，或 y = 0 时 p → 0）：
    - $$\lim_{p \to 1} -\log(p) = 0$$
    - $$\lim_{p \to 0} -\log(1-p) = 0$$

  - 对于错误预测（y = 1 时 p → 0，或 y = 0 时 p → 1）：
    - $$\lim_{p \to 0} -\log(p) = +\infty$$
    - $$\lim_{p \to 1} -\log(1-p) = +\infty$$

  ![Log Loss 性能图示](images/image-1.png)

**结论**：对于高置信度错误预测的惩罚远大于对高置信度正确预测的奖励。

## 数据处理与对话数据表示

原数据格式中的 `prompt`、`response_a` 和 `response_b` 分别是：

- `prompt`: ["prompt_1", "prompt_2", …]
- `response_a`: ["response_a_1", "response_a_2", …]
- `response_b`: ["response_b_1", "response_b_2", …]

我们需要将其解析为 JSON 格式，并将其中的字符串部分提取出来，拼接成需要的格式：

| 数据字段   | 原数据格式                                                   | 拼接后格式                                               |
| ---------- | ------------------------------------------------------------ | -------------------------------------------------------- |
| prompt     | ["你好，请问今天天气如何？", "谢谢，那明天呢？"]             | "你好，请问今天天气如何？谢谢，那明天呢？"               |
| response_a | ["今天天气晴朗，气温适宜。", "根据预报，明天多云，有小雨。"] | "今天天气晴朗，气温适宜。根据预报，明天多云，有小雨。"   |
| response_b | ["今日阳光明媚，温度宜人。", "预计明天阴天，可能有零星小雨。"] | "今日阳光明媚，温度宜人。预计明天阴天，可能有零星小雨。" |

更理想的表示方式是：

```
round_1: 
<prompt>: 你好，请问今天天气如何？
<response_a>: 今天天气晴朗，气温适宜。
<response_b>: 今日阳光明媚，温度宜人。

round_2:
<prompt>: 谢谢，那明天呢？
<response_a>: 根据预报，明天多云，有小雨。
<response_b>: 预计明天阴天，可能有零星小雨。
```

这种表示方式的优点是**保持了对话的时序性，有助于理解上下文关系**。

## 数据探索性分析（EDA）

### Tokens 数量分析

基于对话数据的 tokens 数量分布，选择 `3072` 作为最大序列长度。选择这个数字的原因是它是 16 的倍数，优化了 GPU 上的批量计算。如果不提前设定为 16 的倍数，GPU 运行时需要动态处理每一个数据的填充，这可能导致超时问题。

![Token 数量分析](images/image-2.png)

另外，`response_a` 和 `response_b` 的长度分布几乎一致。

![Token 长度分布对比](images/image-3.png)

## 模型选择和微调策略

### 模型选择

最终选择了 `Gemma-2-9b-it` 模型。因为在 Gemma-2 的后训练（post-training）阶段，使用了主办方的 ChatBot-1M 数据集，这意味着模型在特定领域任务中已经具备了较好的适应能力。实验也表明其表现显著优于其他模型。

### LLM 的两阶段训练

1. **预训练阶段 (Pre-training)**

- **目标**: 学习语言的基本结构、语法规则和广泛的知识。
- **数据**: 使用大量的无标注文本数据（如网页、书籍、文章等）。
- **方法**: 自监督学习（如掩码语言模型（MLM）或因果语言模型（CLM））。
- **特点**: 计算密集型，通常需要大量的 GPU 资源和时间（如在5000张 H100 集群上训练超过 1 个月）。

2. **微调阶段 (Fine-tuning)**

- **目标**: 让模型适应特定的任务或领域。
- **数据**: 使用与目标任务相关的标注数据集。
- **方法**: 有监督学习，根据特定任务的输入输出对来调整参数。
- **特点**: 可以显著提高模型在特定任务上的表现，但可能导致模型在其他任务上的性能下降（灾难性遗忘）。

在 Gemma-2 的案例中，后训练（Post-training）可以看作是预训练和微调之间的一个中间步骤，使用了更专业或特定领域的大规模数据集，训练方式接近预训练，旨在增强模型满足人类偏好的能力。

![训练流程图](images/image-5.png)

### 微调方法选择 - Q

LoRA

**QLoRA** 是一种高效的模型微调技术，它结合了量化（Quantization）和低秩适应（LoRA）方法。这种方法允许在有限的计算资源下微调大型语言模型，同时保持接近全参数微调的性能。

- **量化**: 使用 4 位量化来压缩主模型参数，减少内存占用。
- **低秩适应（LoRA）**: 仅训练低秩适应矩阵 `adapter`，降低了训练的计算复杂度。

![QLoRA 结构图](images/image-4.png)

#### 数学原理

QLoRA 的核心原理包括：

1. **量化**：将 32 位浮点参数压缩为 4 位整数，大幅减少内存使用。
   
   数学表示：$$Q(x) = \text{round}(x / \text{scale}) \times \text{scale}$$

2. **低秩适应（LoRA）**：对于权重矩阵 W，LoRA 添加一个低秩更新：
   
   $$W' = W + BA$$

   其中 $$B \in \mathbb{R}^{d \times r}$$，$$A \in \mathbb{R}^{r \times k}$$，$$r$$ 是秩（通常远小于 $$d$$ 和 $$k$$）。

3. **反向传播**：在训练过程中，梯度仅通过 LoRA 参数（A 和 B）传播，原始模型参数保持冻结。

#### 关键参数和影响

1. **量化位数**：影响模型精度。
2. **LoRA 秩（r）**：较大的 r 提供更多适应能力，但增加参数量。
3. **alpha**：影响训练稳定性和收敛速度，作用等同于学习率。
4. **适应层选择**：决定哪些层应用 LoRA，影响模型适应能力。

### 初始化 LoRA 参数：PISSA

**PISSA（Parameter-Efficient Initialization for Sequentially Stacked Adaptation）** 是一种接近全参数微调性能的初始化方法：

- **原理**：利用预训练模型的参数分布来初始化 LoRA 权重。
- **方法**：
  - 对预训练权重进行 SVD 分解。
  - 使用分解得到的矩阵初始化 LoRA 的 A 和 B。

  ```python
  U, S, Vh = torch.svd(weight)
  lora_A = U[:, :rank] * torch.sqrt(S[:rank])
  lora_B = torch.sqrt(S[:rank]).unsqueeze(0) * Vh[:rank, :]
  ```

- **优势**：提供更好的起始点，加速收敛，改善最终性能，尤其在数据量少的场景下效果更佳。

## 训练和迭代过程

我们经历了五轮迭代，不断优化我们的模型和训练方法。

1. **额外数据集的利用**  
   使用了两个额外数据集：一个是 157k 由 GPT-4 打分的合成数据集，另一个是主办方 2023 年发布的 33k 对话数据。经过分析，我们发现 157k 数据与比赛数据存在显著的分布偏移，因此仅使用 33k 数据与比赛数据进行混合训练。

2. **测试时增强 (TTA)**  
   我们通过在推理时交换 response_a 和 response_b 的顺序，并对结果取平均，抵消了模型对先出现 response 的偏好，从而减少了预测偏差。

3. **设置最大 Tokens 为 3072**  
   经消融实验，3072 是最优的参数设置，可以最大限度地提高模型性能。

4. **LoRA Rank = 64**  
   实验证明 Rank 64 可以实现接近全参数微调的效果，同时保持计算可行性。

5. **替换 Score 层**  
   将单层线性 Score 层替换为三层线性层，进一步改善了模型的适应性和性能表现。

## 未成功的方法

- **LESS 数据筛选方法**：导致模型过拟合，产生过高的置信度预测。
- **EMO 损失**：在 SFT 中有效缓解置信度过高问题，但无法解决 token 概率分布约束问题。
- **NEFTune**：在数据去重清洗后对模型无显著影响。
- **标签平滑（Label Smoothing）**：对小模型有效，但在大型模型中表现不佳。

## 额外细节

1. **SFT LoRA 微调**：只需微调 `q`、`k`、`v` 模块。
2. **分类任务微调**：还需微调 `o` 和 `gate` 模块，以提高分类能力。
3. **数据格式兼容性**：LoRA 可以适应不同的数据格式，快速适应新任务。
4. **加速注意力计算**：使用 Flash Attention，加速计算，无性能损失。

![注意力计算优化](images/image-6.png)

## 可优化点

1. **更多数据**: 对 1M 数据打伪标签并训练，进一步提高模型的泛化能力。
2. **EMA（指数移动平均）**: 在单个模型内集成多个模型权重，进一步提高模型性能。
3. **Rdrop**: 利用对比学习降低 dropout 的影响，增强模型鲁棒性。
4. **使用 8 位量化**: 提高推理速度，支持更长的序列长度。
5. **增加两个预测头**: 预测 response_a 和 response_b 的模型来源，为模型提供更多上下文信息。

## 总结

- **多模型测试与选择**：通过初期测试发现大模型更适合任务，最终选择了 Gemma-2-9b-it 模型。
- **创新性微调方法**：使用 QLoRA 进行高效微调，平衡计算成本与性能。
- **数据增强与预处理策略**：利用额外数据集和测试时增强（TTA）技术，确保模型的鲁棒性和准确性。
- **优化与实验迭代**：多轮实验迭代，优化了 token 长度、LoRA Rank 和模型结构。
- **未来优化方向**：在数据扩展、模型集成和推理效率上还有进一步提升空间。